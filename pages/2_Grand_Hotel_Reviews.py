
import json
import time
import datetime as dt
import numpy as np
import pandas as pd
import streamlit as st
import requests
from typing import Optional
from transformers import pipeline
import matplotlib.pyplot as plt
import re

st.title("ğŸ¨ åœ“å±±å¤§é£¯åº—ï½œä½å®¿è©•è«–æƒ…ç·’åˆ†æï¼ˆDemoï¼‰")
st.caption("æ”¯æ´ Google Places API æ“·å–è©•è«–ï¼Œæˆ–ä¸Šå‚³ CSVï¼ˆtext[, rating, time]ï¼‰ã€‚åƒ…ä¾›æ•™å­¸ç¤ºç¯„ã€‚")

with st.sidebar:
    st.header("è³‡æ–™ä¾†æº")
    mode = st.selectbox("é¸æ“‡ä¾†æº", ["Google Places API", "ä¸Šå‚³ CSV"])
    if mode == "Google Places API":
        api_key = st.text_input("Google Places API Keyï¼ˆä¸æœƒå„²å­˜ï¼‰", type="password")
        query = st.text_input("æœå°‹é—œéµå­—", value="åœ“å±±å¤§é£¯åº— å°åŒ—")
        max_reviews = st.slider("æœ€å¤šè©•è«–æ•¸ï¼ˆå˜—è©¦æŠ“å–ï¼‰", min_value=10, max_value=200, value=80, step=10)
        fetch_btn = st.button("ğŸ” æœå°‹ä¸¦æŠ“å–è©•è«–")
    else:
        uploaded = st.file_uploader("ä¸Šå‚³ CSVï¼ˆéœ€æœ‰ text æ¬„ï¼‰", type=["csv"])
        fetch_btn = st.button("ğŸ“¤ è®€å– CSV")

    st.header("åˆ†æè¨­å®š")
    half_life = st.slider("æ™‚é–“åŠè¡°æœŸï¼ˆå¤©ï¼‰", min_value=1, max_value=30, value=5, step=1)
    pos_th = st.slider("åå¥½é–€æª»ï¼ˆ>ï¼‰", min_value=0.2, max_value=3.0, value=0.8, step=0.1)
    neg_th = st.slider("åå·®é–€æª»ï¼ˆ<ï¼‰", min_value=-3.0, max_value=-0.2, value=-0.8, step=0.1)

def is_chinese(s: str) -> bool:
    return bool(re.search(r"[\\u4e00-\\u9fff]", s or ""))

@st.cache_resource(show_spinner=False)
def get_pipelines(device: int = -1):
    zh_model = "uer/roberta-base-finetuned-dianping-chinese"
    multi_model = "cardiffnlp/twitter-xlm-roberta-base-sentiment"
    zh_clf = pipeline("text-classification", model=zh_model, tokenizer=zh_model, device=device)
    multi_clf = pipeline("text-classification", model=multi_model, tokenizer=multi_model, device=device, top_k=None)
    return zh_clf, multi_clf

def classify_text(zh_clf, multi_clf, text: str):
    try:
        if is_chinese(text):
            out = zh_clf(text[:512])[0]
            label = out.get("label", "").lower()  # positive/negative
            score = float(out.get("score", 0.0))
        else:
            out = multi_clf(text[:512])
            if isinstance(out, list):
                best = sorted(out, key=lambda x: -float(x["score"]))[0]
            else:
                best = out
            label = str(best.get("label", "")).lower()  # positive/negative/neutral
            score = float(best.get("score", 0.0))
        return label, score, None
    except Exception as e:
        return None, None, str(e)

def sent_to_num(label: str) -> float:
    if label == "positive":
        return 1.0
    if label == "negative":
        return -1.0
    return 0.0

def parse_time(ts: Optional[str]) -> pd.Timestamp:
    if ts is None:
        return pd.NaT
    try:
        return pd.to_datetime(ts, utc=True).tz_convert(tz=None)
    except Exception:
        try:
            return pd.to_datetime(ts)
        except Exception:
            return pd.NaT

def time_decay(ts: pd.Timestamp, now: pd.Timestamp, half_life_days: float) -> float:
    if pd.isna(ts):
        return 1.0
    days = max((now - ts).total_seconds() / 86400.0, 0.0)
    lam = np.log(2) / float(half_life_days)
    return float(np.exp(-lam * days))

def places_text_search(api_key: str, text_query: str):
    url = "https://places.googleapis.com/v1/places:searchText"
    headers = {
        "X-Goog-Api-Key": api_key,
        "X-Goog-FieldMask": "places.id,places.displayName"
    }
    payload = {"textQuery": text_query}
    resp = requests.post(url, headers=headers, json=payload, timeout=20)
    if resp.status_code != 200:
        raise RuntimeError(f"TextSearch HTTP {resp.status_code}: {resp.text[:200]}")
    data = resp.json()
    return data.get("places", [])

def places_details_reviews(api_key: str, place_id: str, max_reviews: int = 100):
    url = f"https://places.googleapis.com/v1/places/{place_id}"
    headers = {
        "X-Goog-Api-Key": api_key,
        "X-Goog-FieldMask": "id,displayName,reviews"
    }
    resp = requests.get(url, headers=headers, timeout=20)
    if resp.status_code != 200:
        raise RuntimeError(f"Details HTTP {resp.status_code}: {resp.text[:200]}")
    data = resp.json()
    reviews = data.get("reviews", [])[:max_reviews]
    rows = []
    for r in reviews:
        text = r.get("originalText", {}).get("text") or r.get("text", "")
        rating = r.get("rating")
        time_str = r.get("publishTime")
        rows.append({"text": text, "rating": rating, "time": time_str})
    return pd.DataFrame(rows)

def load_csv(file) -> pd.DataFrame:
    df = pd.read_csv(file)
    if "text" not in df.columns:
        raise ValueError("CSV éœ€åŒ…å« text æ¬„")
    keep = [c for c in ["text","rating","time"] if c in df.columns]
    return df[keep].copy()

if fetch_btn:
    if mode == "Google Places API":
        if not api_key:
            st.error("è«‹è¼¸å…¥ Google Places API Keyã€‚")
            st.stop()
        try:
            st.info("æœå°‹åœ°é»â€¦")
            places = places_text_search(api_key, query)
            if not places:
                st.error("æ‰¾ä¸åˆ°ç¬¦åˆçš„åœ°é»ï¼Œè«‹èª¿æ•´é—œéµå­—ã€‚")
                st.stop()
            place = places[0]
            place_id = place.get("id")
            name = place.get("displayName", {}).get("text", "Unknown")
            st.success(f"æ‰¾åˆ°åœ°é»ï¼š{name}ï¼ˆ{place_id}ï¼‰")

            st.info("æŠ“å–è©•è«–â€¦")
            df = places_details_reviews(api_key, place_id, max_reviews=max_reviews)
            if df.empty:
                st.warning("æ²’æœ‰æŠ“åˆ°è©•è«–ï¼Œå¯èƒ½å—é…é¡æˆ–æ¬„ä½é®ç½©é™åˆ¶ã€‚è«‹ç¨å¾Œå†è©¦ï¼Œæˆ–æ”¹ç”¨ CSV ä¸Šå‚³ã€‚")
                st.stop()
        except Exception as e:
            st.error(f"æŠ“å–å¤±æ•—ï¼š{e}")
            st.stop()
    else:
        if not uploaded:
            st.error("è«‹ä¸Šå‚³ CSVã€‚")
            st.stop()
        try:
            df = load_csv(uploaded)
            st.success(f"è®€å– {len(df)} ç­†è©•è«–ã€‚")
        except Exception as e:
            st.error(f"CSV è®€å–éŒ¯èª¤ï¼š{e}")
            st.stop()

    with st.spinner("è¼‰å…¥æƒ…ç·’æ¨¡å‹ä¸¦åˆ†æâ€¦ï¼ˆç¬¬ä¸€æ¬¡è¼ƒä¹…ï¼‰"):
        zh_clf, multi_clf = get_pipelines(device=-1)

    labels, scores, errs = [], [], []
    for _, r in df.iterrows():
        label, score, err = classify_text(zh_clf, multi_clf, str(r.get("text","")))
        labels.append(label); scores.append(score); errs.append(err)
    df["label"] = labels; df["score"] = scores; df["error"] = errs
    df = df[df["label"].notna()].copy()

    df["sent_num"] = df["label"].map(lambda x: 1.0 if x=="positive" else (-1.0 if x=="negative" else 0.0)) * df["score"]
    now = pd.Timestamp.now(tz=None)
    df["t"] = df.get("time", pd.Series([None]*len(df))).apply(parse_time)
    df["w"] = df["t"].apply(lambda ts: time_decay(ts, now, half_life))
    df["weighted_sent"] = df["sent_num"] * df["w"]

    total_score = float(df["weighted_sent"].sum())
    if total_score > pos_th:
        outlook = "Goodï¼ˆåå¥½ï¼‰"; badge = "âœ…"
    elif total_score < neg_th:
        outlook = "Badï¼ˆåå·®ï¼‰"; badge = "âš ï¸"
    else:
        outlook = "Neutralï¼ˆä¸­æ€§ï¼‰"; badge = "â¸ï¸"

    c1, c2 = st.columns(2)
    c1.metric("è©•è«–æ•¸ï¼ˆæœ‰æ•ˆï¼‰", len(df))
    c2.metric("åŠ æ¬Šæƒ…ç·’åˆ†æ•¸", f"{total_score:.3f}")
    st.metric("ç¸½é«”åˆ¤æ–·", f"{badge} {outlook}")

    st.divider()
    st.subheader("æ¯æ—¥å¹³å‡æƒ…ç·’")
    df["date"] = df["t"].dt.date
    daily = df.dropna(subset=["date"]).groupby("date")["sent_num"].mean().reset_index()
    if not daily.empty:
        plt.figure(figsize=(8,4))
        plt.plot(daily["date"], daily["sent_num"], marker="o")
        plt.xticks(rotation=45)
        plt.title("æ¯æ—¥å¹³å‡æƒ…ç·’")
        plt.xlabel("Date"); plt.ylabel("Avg Sentiment (-1~1)")
        plt.grid(True)
        st.pyplot(plt.gcf()); plt.close()
    else:
        st.write("ç¼ºå°‘æ™‚é–“è³‡è¨Šï¼Œç„¡æ³•ç¹ªåœ–ã€‚")

    st.subheader("æ˜ç´°è³‡æ–™")
    show_cols = [c for c in ["t","label","score","sent_num","w","weighted_sent","rating","text"] if c in df.columns]
    st.dataframe(df.sort_values("t", ascending=False)[show_cols], use_container_width=True)

    st.download_button(
        label="ä¸‹è¼‰çµæœ CSV",
        data=df.sort_values("t", ascending=False)[show_cols].to_csv(index=False).encode("utf-8-sig"),
        file_name="grand_hotel_reviews_sentiment.csv",
        mime="text/csv"
    )

    st.caption("æ¨¡å‹ï¼šä¸­æ–‡ `uer/roberta-base-finetuned-dianping-chinese`ï¼›å¤šèª `cardiffnlp/twitter-xlm-roberta-base-sentiment`ã€‚æ­¤å·¥å…·åƒ…ä¾›æ•™å­¸ç¤ºç¯„ã€‚")
